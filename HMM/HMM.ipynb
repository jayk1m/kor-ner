{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# def flatten_list(lists):\n",
    "#     return functools.reduce(operator.iconcat, lists, [])\n",
    "\n",
    "# def counter_sort(counter):\n",
    "#     return sorted(counter.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "# import time\n",
    "# start_time = time.time()\n",
    "# main()\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import itertools\n",
    "import operator\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "# transform data json to text file\n",
    "def write_data(json_file, N1, N2, to_file):\n",
    "    # load json file to dictionary\n",
    "    with open(json_file, \"r\", encoding='utf-8') as read_file:\n",
    "        data = json.load(read_file)\n",
    "    # data string\n",
    "    sents = data[\"sentence\"]\n",
    "    res, temp = \"\", 0\n",
    "    for n in range(N1 - 1, N2):\n",
    "        for dict in sents[n][\"morp\"]:\n",
    "            for dt in sents[n][\"NE\"]:\n",
    "                if dict[\"lemma\"] in dt[\"text\"]:\n",
    "                    res += dict[\"lemma\"] + '\\t' + dt['type'] + '\\n'\n",
    "                    temp = 1\n",
    "                    break\n",
    "            if temp == 0: \n",
    "                res += dict[\"lemma\"] + '\\t' + 'O' + '\\n'\n",
    "            temp = 0\n",
    "        res += '\\n'\n",
    "    # write data to file\n",
    "    with open(to_file, 'w', encoding='utf-8') as f:\n",
    "        res = res.rstrip() + '\\n'\n",
    "        f.write(res)\n",
    "        \n",
    "# transform data json and write only the words (not the tags) to text file\n",
    "def write_data_words(json_file, N1, N2, to_file):\n",
    "    # load json file to dictionary\n",
    "    with open(json_file, \"r\", encoding='utf-8') as read_file:\n",
    "        data = json.load(read_file)\n",
    "    # data string\n",
    "    sents = data[\"sentence\"]\n",
    "    res = \"\"\n",
    "    for n in range(N1 - 1, N2):\n",
    "        for dict in sents[n][\"morp\"]:\n",
    "            res += dict[\"lemma\"] + '\\n'\n",
    "        res += '\\n'\n",
    "    # write data to file\n",
    "    with open(to_file, 'w', encoding='utf-8') as f:\n",
    "        res = res.rstrip() + '\\n'\n",
    "        f.write(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get emission probability only dependent on the current tag\n",
    "# Laplace smoothing added\n",
    "def get_emission(word, tag, helper1, helper2):\n",
    "    V = len(lexicon)\n",
    "    return (helper1[(word, tag)] + 1) / float(helper2[tag] + V)\n",
    "\n",
    "# get transition probability from tag1 to tag2\n",
    "def get_transition(tag1, tag2):\n",
    "    trans = []\n",
    "    tags_all = [x if len(x) > 1 else x + ['<END>'] for x in lexicon_all]\n",
    "    tags_all = [row[1] for row in tags_all]\n",
    "    for i in range(len(tags_all) - 1):\n",
    "        seq = ' '.join(tags_all[i:i+2])\n",
    "        trans.append(seq)\n",
    "    return Counter(trans)[tag1+' '+tag2] / float(Counter(tags_all)[tag1])\n",
    "\n",
    "# get starting probability based on first tags of sentences\n",
    "def get_start_prob(tag):\n",
    "    start = [lexicon_all[i+1] for i in range(len(lexicon_all) - 1) if lexicon_all[i] == [''] or i == 0]\n",
    "    return Counter([row[1] for row in start])[tag] / float(len(start))\n",
    "\n",
    "# TEST\n",
    "# get_emission('일', 'DT', helper1, helper2)\n",
    "# get_emission('김연아', 'PS', helper1, helper2)\n",
    "# get_transition('O', 'PS')\n",
    "# for tag in tags:\n",
    "#     print(tag)\n",
    "#     print(get_start_prob(tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the above probabilities to csv\n",
    "def store_to_csv():\n",
    "    # transition matrix\n",
    "    temp = np.zeros([len(tags), len(tags)])\n",
    "    for i in range(len(tags)):\n",
    "        for j in range(len(tags)):\n",
    "            temp[i, j] = get_transition(tags[i], tags[j])\n",
    "    np.savetxt(\"stored_transition.csv\", temp, delimiter=\",\")\n",
    "    # starting probability matrix\n",
    "    temp = np.zeros([1, len(tags)])\n",
    "    for i in range(len(tags)):\n",
    "        temp[0, i] = get_start_prob(tags[i])\n",
    "    np.savetxt(\"stored_start_prob.csv\", temp, delimiter=\",\")\n",
    "    # emission matrix\n",
    "    temp = np.zeros([len(tags), len(lexicon)])\n",
    "    for t in range(len(tags)):\n",
    "        for l in range(len(lexicon)):\n",
    "            temp[t, l] = get_emission(lexicon[l], tags[t], helper1, helper2)\n",
    "    np.savetxt(\"stored_emission.csv\", temp, delimiter=\",\")\n",
    "    \n",
    "def get_stored(file1, file2, file3):\n",
    "    stored_start_prob = np.genfromtxt(file1, delimiter=',')\n",
    "    stored_transition = np.genfromtxt(file2, delimiter=',')\n",
    "    stored_emission = np.genfromtxt(file3, delimiter=',')\n",
    "    # adding a column of uniform probability (for unseen words) to emission matrix (a la Laplace smoothing)\n",
    "    avg_per_tag, V = len(lexicon_split) / len(tags), len(lexicon)\n",
    "    stored_emission = np.append(stored_emission, [[1 / float(avg_per_tag + V)] for x in range(len(tags))], axis=1)\n",
    "    return stored_start_prob, stored_transition, stored_emission\n",
    "    \n",
    "# store_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A and B are transition and emission matrices, respectively\n",
    "# initial (starting probability) and text are lists\n",
    "def viterbi_sentence(A, B, initial, text):\n",
    "    def index_helper(w):\n",
    "        try:\n",
    "            return lexicon.index(w)\n",
    "        except ValueError:\n",
    "            return len(lexicon)\n",
    "    M = A.shape[0]    \n",
    "    N = len(text)\n",
    "    textn = [index_helper(x) for x in text]\n",
    "    # initialization\n",
    "    best_prob = np.zeros([M, N])\n",
    "    best_tags = np.zeros([M, N - 1])\n",
    "    best_prob[:, 0] = np.multiply(initial, B[:, textn[0]])\n",
    "    # dynamic programming for best_prob (which is our Viterbi function)\n",
    "    for i in range(1, N):\n",
    "        for t in range(M):\n",
    "            temp = np.multiply(best_prob[:, i-1], A[:, t])\n",
    "            best_prob[t, i] = np.max(temp) * B[t, textn[i]]\n",
    "            best_tags[t, i-1] = np.argmax(temp)\n",
    "    # recovering sequence based on max probability\n",
    "    max_temp = np.zeros([1, N])\n",
    "    max_temp[0, -1] = np.argmax(best_prob[:, -1])\n",
    "    # backtracking\n",
    "    for i in range(N - 2, -1, -1):\n",
    "        max_temp[0, i] = best_tags[int(max_temp[0, i+1]), i]\n",
    "    opt_tags = max_temp.astype(int)[0]\n",
    "    return np.stack([text, [tags[x] for x in opt_tags]]).T.tolist()\n",
    "\n",
    "# can input a file (not just a sentence) using this function\n",
    "def viterbi_file(A, B, initial, file_from, file_to):\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"---- NER tagging using Hidden Markov Model and Viterbi ----\")\n",
    "    lexicon_dev = [line.rstrip('\\n') for line in open(file_from, encoding='utf-8')]\n",
    "    lexicon_dev = [list(y) for x, y in itertools.groupby(lexicon_dev, lambda z: z == '') if not x]\n",
    "    viterbi_res = [viterbi_sentence(A, B, initial, sent) for sent in lexicon_dev]\n",
    "    res = '\\n'.join([''.join(['\\t'.join(w) + '\\n' for w in viterbi_res[sent]]) for sent in range(len(viterbi_res))])\n",
    "    with open(file_to, \"w\", encoding='utf-8') as f:\n",
    "        f.write(res)\n",
    "#     return viterbi_res\n",
    "    print(\"-- Successfully exported to\", file_to, \"--\")\n",
    "    \n",
    "    print(\"---- Took %s seconds ----\" % (time.time() - start_time))\n",
    "\n",
    "# TEST\n",
    "# text = ['한편', '지나', 'ㄴ', '해', 'K', '리그', '챔피언', '포항', '은', '중국', '창춘', '스타디움', '에서', '열리', 'ㄴ', '지난해', '중국', '슈퍼', '리그', '우승팀', '창춘', '과', '의', 'E', '조', '3', '차전', '원정', '경기', '에서', '후반', '40', '분', '결승골', '을', '내주', '고', '0', '-', '1', '로', '패하', '았', '다', '.']\n",
    "# viterbi_sentence(stored_transition, stored_emission, stored_start_prob, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "#\n",
    "# scorer for NLP class Spring 2016\n",
    "# ver.1.0\n",
    "#\n",
    "# score a key file against a response file\n",
    "# both should consist of lines of the form:   token \\t tag\n",
    "# sentences are separated by empty lines\n",
    "#\n",
    "def score_old (keyFileName, responseFileName):\n",
    "    keyFile = open(keyFileName, 'r', encoding='utf-8')\n",
    "    key = keyFile.readlines()\n",
    "    responseFile = open(responseFileName, 'r', encoding='utf-8')\n",
    "    response = responseFile.readlines()\n",
    "    if len(key) != len(response):\n",
    "        print (\"length mismatch between key and submitted file\")\n",
    "        exit()\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    for i in range(len(key)):\n",
    "        key[i] = key[i].rstrip('\\n')\n",
    "        response[i] = response[i].rstrip('\\n')\n",
    "        if key[i] == \"\":\n",
    "            if response[i] == \"\":\n",
    "                continue\n",
    "            else:\n",
    "                print (\"sentence break expected at line \" + str(i))\n",
    "                exit()\n",
    "        keyFields = key[i].split('\\t')\n",
    "        if len(keyFields) != 2:\n",
    "            print (\"format error in key at line \" + str(i) + \":\" + key[i])\n",
    "            exit()\n",
    "        keyToken = keyFields[0]\n",
    "        keyPos = keyFields[1]\n",
    "        responseFields = response[i].split('\\t')\n",
    "        if len(responseFields) != 2:\n",
    "            print (\"format error at line \" + str(i))\n",
    "            exit()\n",
    "        responseToken = responseFields[0]\n",
    "        responsePos = responseFields[1]\n",
    "        if responseToken != keyToken:\n",
    "            print (\"token mismatch at line \" + str(i))\n",
    "            exit()\n",
    "        if responsePos == keyPos:\n",
    "            correct = correct + 1\n",
    "        else:\n",
    "            incorrect = incorrect + 1\n",
    "    print (str(correct) + \" out of \" + str(correct + incorrect) + \" tags correct\")\n",
    "    accuracy = 100.0 * correct / (correct + incorrect)\n",
    "    print (\"  accuracy: %f\" % accuracy)\n",
    "    \n",
    "# score_old ('HMM_test_correct.data','HMM_test.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(file_correct, file):\n",
    "    res1 = [line.rstrip('\\n').split('\\t')[1] for line in open(file_correct, encoding='utf-8') if line != '\\n']\n",
    "    c1 = Counter(res1)\n",
    "    res2 = [line.rstrip('\\n').split('\\t')[1] for line in open(file, encoding='utf-8') if line != '\\n']\n",
    "    c2 = Counter(res2)\n",
    "    c3 = Counter(list(zip(res1, res2)))\n",
    "    res = []\n",
    "    for tag in ['O', 'OG', 'DT', 'PS', 'LC', 'TI']:\n",
    "        prec = c3[(tag, tag)] / c2[tag]\n",
    "        rec = c3[(tag, tag)] / c1[tag]\n",
    "        res += [[prec, rec, 2 * prec * rec / float(prec + rec)]]\n",
    "    df = pd.DataFrame(res, index=['O', 'OG', 'DT', 'PS', 'LC', 'TI'], columns=['Precision', 'Recall', 'F1'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hmm = 'HMM_train.data'\n",
    "lexicon_split = [line.rstrip('\\n').split('\\t') for line in open(data_hmm, encoding='utf-8') if line.rstrip('\\n') != '']\n",
    "lexicon_all = [line.rstrip('\\n').split('\\t') for line in open(data_hmm, encoding='utf-8')]\n",
    "lexicon = sorted({row[0] for row in lexicon_split})\n",
    "tags = sorted({row[1] for row in lexicon_split})\n",
    "\n",
    "helper1, helper2 = Counter([tuple(x) for x in lexicon_split]), Counter([row[1] for row in lexicon_split])\n",
    "file1, file2, file3 = 'stored_start_prob.csv', 'stored_transition.csv', 'stored_emission.csv'\n",
    "stored_start_prob, stored_transition, stored_emission = get_stored(file1, file2, file3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Starts Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in training data: 12142\n",
      "NER tags in training data: ['DT', 'LC', 'O', 'OG', 'PS', 'TI']\n",
      "---- NER tagging using Hidden Markov Model and Viterbi ----\n",
      "-- Successfully exported to HMM_test.data --\n",
      "---- Took 4.197366237640381 seconds ----\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>0.883633</td>\n",
       "      <td>0.988008</td>\n",
       "      <td>0.932910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OG</th>\n",
       "      <td>0.808429</td>\n",
       "      <td>0.230475</td>\n",
       "      <td>0.358691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DT</th>\n",
       "      <td>0.845038</td>\n",
       "      <td>0.771429</td>\n",
       "      <td>0.806557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PS</th>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.029499</td>\n",
       "      <td>0.055866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LC</th>\n",
       "      <td>0.752525</td>\n",
       "      <td>0.318376</td>\n",
       "      <td>0.447447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TI</th>\n",
       "      <td>0.752475</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.700461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Precision    Recall        F1\n",
       "O    0.883633  0.988008  0.932910\n",
       "OG   0.808429  0.230475  0.358691\n",
       "DT   0.845038  0.771429  0.806557\n",
       "PS   0.526316  0.029499  0.055866\n",
       "LC   0.752525  0.318376  0.447447\n",
       "TI   0.752475  0.655172  0.700461"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Number of unique words in training data:', len(lexicon))\n",
    "print('NER tags in training data:', tags)\n",
    "\n",
    "# can segment the json file differently if desirable\n",
    "# write_data(\"NEtaggedCorpus_train.json\", 1, 3000, data_hmm)\n",
    "# write_data_words(\"NEtaggedCorpus_train.json\", 3001, 3555, \"HMM_test.words\")\n",
    "# write_data(\"NEtaggedCorpus_train.json\", 3001, 3555, 'HMM_test_correct.data')\n",
    "\n",
    "viterbi_file(stored_transition, stored_emission, stored_start_prob, 'HMM_test.words', 'HMM_test.data')\n",
    "score('HMM_test_correct.data', 'HMM_test.data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
